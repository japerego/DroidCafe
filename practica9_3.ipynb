{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/japerego/DroidCafe/blob/master/practica9_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHXROQV9uZ2n"
      },
      "source": [
        "# Práctica 9 Parte 3: Desarrollando un modelo de lenguaje para generar texto\n",
        "\n",
        "Un modelo de lenguaje puede predecir la siguiente palabra de una secuencia basándose en palabras observadas anteriormente. Las redes neuronales son el método más utilizado para desarrollar este tipo de modelos porque pueden usar una representación donde palabras con significados similares tienen representaciones similares. \n",
        "\n",
        "En esta parte de la práctica vamos a ver cómo generar uno de esos modelos. \n",
        "\n",
        "Este notebook está basado en el libro Deep Learning for Natural Language Processing de Jason Brownlee. \n",
        "\n",
        "Es importante que tengas activado el uso de **GPU** en el notebook de colab (menú Edit -> Notebook Settings -> Hardware accelerator)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7fHn2VKwO0E"
      },
      "source": [
        "## La República de Platón\n",
        "\n",
        "Nuestro modelo de lenguaje va a estar basado en la república de Platón. Este libro está estructurado en forma de una conversación que trata el tema del orden y la justicia dentro de una ciudad. El texto completo está disponible para el dominio público dentro del [proyecto Gutenberg](http://www.gutenberg.org/).\n",
        "\n",
        "Este libro de Platón está disponible en varios formatos en el [proyecto Gutenberg](http://www.gutenberg.org/cache/epub/1497/pg1497.txt). La versión que nos interesa a nosotros es la versión ASCII del libro. Con la siguiente instrucción puedes descargar el libro donde se han eliminado la portada y la contraportada. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HpmhqeiuuFYV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1348265-617f-4fe0-aa8b-c6f570ffd1e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-28 15:14:40--  https://raw.githubusercontent.com/ts1819/datasets/master/practica5/republic.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 657826 (642K) [text/plain]\n",
            "Saving to: ‘republic.txt’\n",
            "\n",
            "republic.txt        100%[===================>] 642.41K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2022-05-28 15:14:42 (184 MB/s) - ‘republic.txt’ saved [657826/657826]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/ts1819/datasets/master/practica5/republic.txt -O republic.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb_hckdhxWdz"
      },
      "source": [
        "## Preparación de los datos\n",
        "\n",
        "Vamos a preparar los datos para construir nuestro modelo. \n",
        "\n",
        "### Revisando el texto\n",
        "\n",
        "Vamos a comenzar revisando parte del texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kn3g3XJYxSTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b955c439-be99-4422-f52a-ba46686eee70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOOK I.\r\n",
            "\r\n",
            "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\r\n",
            "that I might offer up my prayers to the goddess (Bendis, the Thracian\r\n",
            "Artemis.); and also because I wanted to see in what manner they would\r\n",
            "celebrate the festival, which was a new thing. I was delighted with the\r\n",
            "procession of the inhabitants; but that of the Thracians was equally,\r\n",
            "if not more, beautiful. When we had finished our prayers and viewed the\r\n",
            "spectacle, we turned in the direction of the city; and at that instant\r\n",
            "Polemarchus the son of Cephalus chanced to catch sight of us from a\r\n",
            "distance as we were starting on our way home, and told his servant to\r\n",
            "run and bid us wait for him. The servant took hold of me by the cloak\r\n",
            "behind, and said: Polemarchus desires you to wait.\r\n",
            "\r\n",
            "I turned round, and asked him where his master was.\r\n",
            "\r\n",
            "There he is, said the youth, coming after you, if you will only wait.\r\n",
            "\r\n",
            "Certainly we will, said Glaucon; and in a few minutes Polemarchus\r\n",
            "appeared, and with him Adeimantus, Glaucon's brother, Niceratus the son\r\n",
            "of Nicias, and several others who had been at the procession.\r\n",
            "\r\n",
            "Polemarchus said to me: I perceive, Socrates, that you and your\r\n",
            "companion are already on your way to the city.\r\n",
            "\r\n",
            "You are not far wrong, I said.\r\n",
            "\r\n",
            "But do you see, he rejoined, how many we are?\r\n",
            "\r\n",
            "Of course.\r\n"
          ]
        }
      ],
      "source": [
        "!head -30 republic.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4uFv8RWzVn-"
      },
      "source": [
        "A partir de un rápido vistazo al fragmento de texto anterior podemos ver ciertas cuestiones que tendremos que procesar:\n",
        "- Las cabeceras de los capítulos.\n",
        "- Muchos signos de puntuación.\n",
        "- Nombres extraños.\n",
        "- Algunos monólogos muy largos. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiAfXxOz0GQT"
      },
      "source": [
        "### Cargando el texto\n",
        "\n",
        "El primer paso consiste en cargar el texto en memoria. Podemos desarrollar una pequeña función que se encargue de esto. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_dDTRFi4xmBr"
      },
      "outputs": [],
      "source": [
        "def load_doc(filename):\n",
        "  # Abrimos el fichero en modo lectura\n",
        "  file = open(filename,'r')\n",
        "  # Leemos el texto completo\n",
        "  text = file.read()\n",
        "  # Cerramos el fichero\n",
        "  file.close()\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SdKtPFM0aZ-"
      },
      "source": [
        "Usando dicha función podemos cargar nuestro fichero del siguiente modo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x_oUjl_Q0Zgn"
      },
      "outputs": [],
      "source": [
        "in_filename = 'republic.txt'\n",
        "doc = load_doc(in_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVfi8WHI0kKb"
      },
      "source": [
        "Ahora podemos mostrar parte de dicho texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8ZRb4q6W0jQT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67bfd64f-2ed4-45bc-e04d-ba135b1438c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOOK I.\n",
            "\n",
            "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
            "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
            "Artemis.); and also because I wanted to see in what\n"
          ]
        }
      ],
      "source": [
        "print(doc[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj4PZ32G0reo"
      },
      "source": [
        "### Limpiando el texto\n",
        "\n",
        "Ahora necesitamos transformar el texto en bruto a una secuencia de tokens (o palabras) que podamos usar para entrenar nuestro modelo. \n",
        "\n",
        "Vamos a aplicar las siguientes operaciones para limpiar nuestro texto:\n",
        "- Reemplazar todas las ocurrencias de '-' con un espacio en blanco de manera que podamos partir mejor las palabras.\n",
        "- Partir las palabras basándonos en espacios en blanco.\n",
        "- Eliminar todos los símbolos de puntuación.\n",
        "- Eliminar todas las palabras que no son alfabéticas. \n",
        "- Normalizar todas las palabras a minúsculas.\n",
        "\n",
        "La mayoría de estas transformaciones tienen como objetivo reducir el tamaño del vocabulario. Un tamaño de vocabulario excesivamente grande es un problema cuando se intenta crear modelos de lenguaje. Vocabularios pequeños producen modelos más pequeños que se entrenan más rápidos.\n",
        "\n",
        "Vamos a implementar todas las operaciones de limpieza en la siguiente función."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NsZiyRsE0niT"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "def clean_doc(doc):\n",
        "  # Reemplazar '--' con un espacio en blanco ' '\n",
        "  doc = doc.replace('--',' ')\n",
        "  # Partir palabras basándonos en espacios en blanco\n",
        "  tokens = doc.split()\n",
        "  # Vamos a escapar las palabras para poder filtrarlas por caracteres\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # Eliminamos los símbolos de puntuación\n",
        "  tokens = [re_punc.sub('',w) for w in tokens]\n",
        "  # Eliminamos elementos que nos son alfabéticos\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # Convertimos a minúsculas\n",
        "  tokens = [word.lower() for word in tokens]\n",
        "  return tokens\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVV2RsWB2kOD"
      },
      "source": [
        "Procedemos a limpiar nuestro documento y a continuación mostramos algunas estadísticas sobre nuestro vocabulario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wUUCtGKP2jdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba3e2e9-e529-4e26-f9d5-38d6acc9bd80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said']\n"
          ]
        }
      ],
      "source": [
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MOBzRk232uAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23220e76-e198-48c0-afad-01db761db3e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens: 118684\n",
            "Unique Tokens: 7409\n"
          ]
        }
      ],
      "source": [
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' %len(set(tokens)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvkoWbVh25cR"
      },
      "source": [
        "Es decir, nuestro modelo consta de una 7500 palabras. Este tamaño de vocabulario es pequeño y va a ser manejable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odlhVwpz3CQi"
      },
      "source": [
        "### Guardando el texto limpio\n",
        "\n",
        "Vamos a organizar la larga lista de tokens en secuencias de 50 palabras de entrada y 1 palabra de salida (esto servirá para luego entrenar nuestro modelo). \n",
        "\n",
        "Este proceso lo implementamos con la siguiente función."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qtEe2J7E21am"
      },
      "outputs": [],
      "source": [
        "def organize_tokens(tokens,input_len=50,output_len=1):\n",
        "  length = input_len + output_len\n",
        "  sequences = list()\n",
        "  for i in range(length,len(tokens)):\n",
        "    # Elegimos la secuencia de tokens\n",
        "    seq = tokens[i-length:i]\n",
        "    # Convertimos la secuencia en una línea\n",
        "    line = ' '.join(seq)\n",
        "    # Almacenamos el resultado\n",
        "    sequences.append(line)\n",
        "  return sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6upZchl37wM"
      },
      "source": [
        "Organizamos nuestros tokens. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jpcwuFBp35XJ"
      },
      "outputs": [],
      "source": [
        "lines = organize_tokens(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXyDfPjA4Bfr"
      },
      "source": [
        "Ahora vamos a guardar las secuencias en un nuevo fichero para poder cargarlo en el futuro. Para ello nos definimos la siguiente función que guardará cada elemento de la secuencia en una línea del fichero. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SWxPoCvn4AfS"
      },
      "outputs": [],
      "source": [
        "def save_doc(lines,filename):\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(filename,'w')\n",
        "  file.write(data)\n",
        "  file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtdjRviE4Wfd"
      },
      "source": [
        "Podemos llamar a la función anterior para guardar nuestro fichero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rbncNaRo4T74"
      },
      "outputs": [],
      "source": [
        "out_filename = 'republic_sequences.txt'\n",
        "save_doc(lines,out_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giGMZ3yW4m9O"
      },
      "source": [
        "Podemos ver parte de dicho fichero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hitWeMb84e1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92dfabd4-84a2-4266-d394-f137c0687124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was\n",
            "i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted\n",
            "i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with\n",
            "went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the\n",
            "down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession\n"
          ]
        }
      ],
      "source": [
        "!head -5 republic_sequences.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-Do4ht04u3Q"
      },
      "source": [
        "## Entrenando el modelo de lenguaje\n",
        "\n",
        "Vamosa  entrenar ahora nuestro modelo a partir de los datos que hemos preparado. Dicho modelo tendrá ciertas características:\n",
        "- Usará una representación para las palabras de manera que palabras diferentes con significados similares tendrán una representación similar.\n",
        "- La representación será aprendida al mismo tiempo que se aprende el modelo.\n",
        "- Aprenderá a predecir la probabilidad de la siguiente palabra a partir del contexto de las últimas 100 palabras.\n",
        "\n",
        "En concreto para implementar este modelo vamos a usar una capa de Embedding para aprender la representación de las palabras, y una red neuronal recurrente con capas LSTM para predecir nuevas palabras basándonos en el contexto. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss_vMHL65cx2"
      },
      "source": [
        "### Cargando las secuencias\n",
        "\n",
        "Podemos comenzar cargando las secuencias que hemos guardado anteriormente. En este caso este paso no sería necesario ya que el proceso de generación de las secuencias es bastante rápido, pero si estamos trabajando con un dataset más grande sí que puede ser conveniente. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mChmy3q04rJq"
      },
      "outputs": [],
      "source": [
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Au4s6vM7UED"
      },
      "source": [
        "### Codificando las secuencias\n",
        "\n",
        "Las capas de Embedding esperan que las secuencias de entrada estén compuestas de vectores de enteros. Para ello vamos a identificar cada palabra de nuestro vocabulario con un entero único y codificarlo en una secuencia de entrada. En el futuro cuando vayamos a realizar las predicciones tendremos que realizar el proceso inverso.\n",
        "\n",
        "Para llevar a cabo este proceso de tokenización vamos a usar la API de Keras del siguiente modo. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "eVH8p7Cv52Zo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "d_odT9JZ710u"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEQoF2S478z9"
      },
      "source": [
        "Ahora podemos acceder a los identificadores de cada palabra usando el atributo ``word_index`` del objeto ``Tokenizer`` que hemos creado. \n",
        "\n",
        "Además debemos determinar el tamaño de nuestro vocabulario para definir la capa de embedding. En concreto, a las palabras de nuestro vocabulario se les han asignado valores entre 1 y el número total de palabras de nuestro vocabulario. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "byu4Vfe075Aj"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaF1yDTF9elO"
      },
      "source": [
        "### Secuencias de entrada y salida\n",
        "\n",
        "Una vez que tenemos codificadas nuestras secuencias tenemos que separarlas en elementos de entrada ($X$) y de salida ($y$). Después de realizar la separación debemos codificar cada palabra usando el método one-hot. Este proceso lo llevaremos a cabo mediante la función ``to_categorical()`` de Keras.\n",
        "Finalmente necesitamos especificar cómo de largas serán las secuencias de entrada. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SekaGTwl9eCl"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from numpy import array \n",
        "\n",
        "sequences = array(sequences)\n",
        "X,y=sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y,num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvUMEkkh-gSm"
      },
      "source": [
        "### Entrenando el modelo\n",
        "\n",
        "Ahora podemos definir nuestro modelo que constará de una capa de Embedding, seguida de dos capas LSTM y terminando con una red completamente conectada. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cs_7VIsS-ODn"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def define_model(vocab_size,seq_length):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size,50,input_length=seq_length))\n",
        "  model.add(LSTM(100,return_sequences=True))\n",
        "  model.add(LSTM(100))\n",
        "  model.add(Dense(100,activation='relu'))\n",
        "  model.add(Dense(vocab_size,activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMo-mV9E_cco"
      },
      "source": [
        "Pasamos a entrenar nuestro modelo. Como este proceso es bastante costoso (incluso usando GPUs) en la siguiente sección se proporcionan los ficheros necesarios para usar el modelo. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gFxHVuAuCdzC"
      },
      "outputs": [],
      "source": [
        "model = define_model(vocab_size,seq_length)\n",
        "model.fit(X,y,batch_size=128,epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EeUlPGxEifh"
      },
      "source": [
        "Una vez entrenado podemos guardar los pesos del modelo y el tokenizador. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CKc5WObR_maO"
      },
      "outputs": [],
      "source": [
        "model.save_weights('./model.h5', overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kdzX5deoEuiw"
      },
      "outputs": [],
      "source": [
        "from pickle import dump\n",
        "dump(tokenizer, open('tokenizer.pkl','wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rnE1BOhEuIz"
      },
      "source": [
        "## Usando el modelo\n",
        "\n",
        "Como has podido ver en el paso anterior, el proceso de entrenar este tipo de modelos es muy costoso, por lo que puedes descargar los ficheros necesarios para usar el modelo desde el siguiente enlace. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "IJozw5qBFUoO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bbb4a13-9d3e-4a64-e6c4-8ac8ec4a700a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-28 15:27:48--  https://raw.githubusercontent.com/ts1819/datasets/master/practica5/tpu_model.h5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5101248 (4.9M) [application/octet-stream]\n",
            "Saving to: ‘model.h5’\n",
            "\n",
            "model.h5            100%[===================>]   4.86M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-05-28 15:27:49 (193 MB/s) - ‘model.h5’ saved [5101248/5101248]\n",
            "\n",
            "--2022-05-28 15:27:49--  https://raw.githubusercontent.com/ts1819/datasets/master/practica5/tokenizer.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 353379 (345K) [application/octet-stream]\n",
            "Saving to: ‘tokenizer.pkl’\n",
            "\n",
            "tokenizer.pkl       100%[===================>] 345.10K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-05-28 15:27:49 (26.6 MB/s) - ‘tokenizer.pkl’ saved [353379/353379]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/ts1819/datasets/master/practica5/tpu_model.h5 -O model.h5\n",
        "!wget https://raw.githubusercontent.com/ts1819/datasets/master/practica5/tokenizer.pkl -O tokenizer.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbxpdyp4FZ0t"
      },
      "source": [
        "### Cargando los datos\n",
        "\n",
        "Comenzamos cargando nuestros datos al igual que antes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LwREuZMiFfVT"
      },
      "outputs": [],
      "source": [
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQIu2LzlFl5v"
      },
      "source": [
        "Necesitamos este texto para elegir una secuencia de inicio que será la entrada para nuestro modelo. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "a-IslrBxFlnA"
      },
      "outputs": [],
      "source": [
        "seq_length = len(lines[0].split())-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejCQcIN_Fxr2"
      },
      "source": [
        "### Cargando el modelo\n",
        "\n",
        "Vamos a cargar el modelo y a fijar los pesos. Notar que para este paso ya no necesitamos el uso de TPU, y que el modelo podría ser usado en cualquier ordenador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "usmQZBvYF7Q1"
      },
      "outputs": [],
      "source": [
        "model = define_model(vocab_size,seq_length)\n",
        "model.load_weights('./model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uvlOps3GG05"
      },
      "source": [
        "También necesitamos cargar el tokenizador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "xib_xurDGIpL"
      },
      "outputs": [],
      "source": [
        "from pickle import load\n",
        "tokenizer = load(open('tokenizer.pkl','rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XrPda8NGUv7"
      },
      "source": [
        "### Generando texto\n",
        "\n",
        "El primer paso para generar el texto consiste en preparar una entrada, para lo cual elegiremos una línea aleatoria del texto. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1EDkPYM5Gdmz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "165e02ac-5525-450f-b84a-5bb602ba7fc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best he said and now that we know what they are like there is no difficulty in tracing out the sort of life which awaits either of them this i will proceed to describe but as you may think the description a little too coarse i ask you to suppose socrates\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from random import randint\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZO-Teo5Go9O"
      },
      "source": [
        "A continuación podemos generar nuevas palabras una por una. Primero, el texto debe codificarse usando el tokenizer que hemos cargado anteriormente. Ahora el modelo puede predecir nuevas palabras usando el método ``predict_classes()`` que devuelve el índice de la palabra con probabilidad más alta. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mNpLQAtHMEY"
      },
      "source": [
        "Esta palabra se añade a nuestro texto inicial y se repite el proceso. Notar que esta secuencia va a ir creciendo por lo que tendremos que truncarla, para lo que utilizamos la función ``pad_sequences()`` de Keras. Todo este proceso se puede implementar con la siguiente función. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "R4J29cLaHZx-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "def generate_seq(model,tokenizer,seq_length,seed_text,n_words):\n",
        "  result = list()\n",
        "  in_text = seed_text\n",
        "  for _ in range(n_words):\n",
        "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "    encoded = pad_sequences([encoded],maxlen=seq_length,truncating='pre')\n",
        "    yhat = model.predict(encoded,verbose=0)\n",
        "    yhat=np.argmax(yhat,axis=1)\n",
        "    out_word = ''\n",
        "    for word,index in tokenizer.word_index.items():\n",
        "      if index == yhat:\n",
        "        out_word = word\n",
        "        break\n",
        "    in_text += ' ' + out_word\n",
        "    result.append(out_word)\n",
        "  return ' '.join(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h6dQKHIIIot"
      },
      "source": [
        "Ahora podemos generar una nueva secuencia usando el siguiente código. Cada vez que lo ejecutemos obtendremos un resultado distinto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3FUn97EmILPG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eec84d2-e31b-48b4-e9ce-ef752b1d2125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "can tell the use of them still these studies force their way by their natural charm and very likely if they had the help of the state they would some day emerge into light yes he said there is a remarkable charm in them but i do not clearly understand the\n",
            "\n",
            "question is the way of the soul and the other rascalities which is the ordering of the soul and the other allurements of the soul and the other principle of a state and the other rascalities which fills him with lyes and will meddle with laughter and prove a restraint\n"
          ]
        }
      ],
      "source": [
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')\n",
        "generated = generate_seq(model,tokenizer,seq_length,seed_text,50)\n",
        "print(generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlmSJGumIepK"
      },
      "source": [
        "## Ejercicio\n",
        "\n",
        "Elige tu propio libro del proyecto Gutenberg (es posible usar libros en [español](https://www.gutenberg.org/browse/languages/es)) y crea tu propio modelo de lenguaje. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "from pickle import load\n",
        "from pickle import dump\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from numpy import array "
      ],
      "metadata": {
        "id": "vXp-QacEZhex"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "v3jsZvvYI21o"
      },
      "outputs": [],
      "source": [
        "!wget https://www.gutenberg.org/cache/epub/29640/pg29640.txt -o Gutenberg.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -30 pg29640.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-Za_Vr6U3nr",
        "outputId": "50207d46-8b8c-4165-c23e-b1906cbab4b4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿The Project Gutenberg EBook of Germana, by Edmond About\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere at no cost and with\r\n",
            "almost no restrictions whatsoever.  You may copy it, give it away or\r\n",
            "re-use it under the terms of the Project Gutenberg License included\r\n",
            "with this eBook or online at www.gutenberg.org\r\n",
            "\r\n",
            "\r\n",
            "Title: Germana\r\n",
            "\r\n",
            "Author: Edmond About\r\n",
            "\r\n",
            "Translator: Tomás Orts-Ramos\r\n",
            "\r\n",
            "Release Date: August 8, 2009 [EBook #29640]\r\n",
            "\r\n",
            "Language: Spanish\r\n",
            "\r\n",
            "\r\n",
            "*** START OF THIS PROJECT GUTENBERG EBOOK GERMANA ***\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Produced by Chuck Greif and the Online Distributed\r\n",
            "Proofreading Team at https://www.pgdp.net\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_filename = 'pg29640.txt'\n",
        "doc = load_doc(in_filename)"
      ],
      "metadata": {
        "id": "Zt1_1HgrVJgr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2whFGPQvVKJ6",
        "outputId": "c2466984-d043-4af2-880d-7d84d165af9b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿The Project Gutenberg EBook of Germana, by Edmond About\n",
            "\n",
            "This eBook is for the use of anyone anywhere at no cost and with\n",
            "almost no restrictions whatsoever.  You may copy it, give it away or\n",
            "re-use i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "outputId": "c08e90e7-2eb8-4d88-eba5-bb5706db216a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu_gBWz5VaLH"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['project', 'gutenberg', 'ebook', 'of', 'germana', 'by', 'edmond', 'about', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorg', 'title', 'germana', 'author', 'edmond', 'about', 'translator', 'tomás', 'ortsramos', 'release', 'date', 'august', 'ebook', 'language', 'spanish', 'start', 'of', 'this', 'project', 'gutenberg', 'ebook', 'germana', 'produced', 'by', 'chuck', 'greif', 'and', 'the', 'online', 'distributed', 'proofreading', 'team', 'at', 'httpswwwpgdpnet', 'biblioteca', 'de', 'la', 'nación', 'edmundo', 'about', 'germana', 'traducción', 'de', 't', 'ortsramos', 'buenos', 'aires', 'derechos', 'reservados', 'imp', 'de', 'la', 'nación', 'buenos', 'aires', 'indice', 'i', 'el', 'aguinaldo', 'de', 'la', 'duquesa', 'ii', 'petición', 'de', 'matrimonio', 'iii', 'la', 'boda', 'iv', 'viaje', 'a', 'italia', 'v', 'el', 'duque', 'vi', 'cartas', 'de', 'corfú', 'vii', 'el', 'nuevo', 'doméstico', 'viii', 'los', 'buenos', 'tiempos', 'ix', 'cartas', 'de', 'china', 'y', 'de', 'parís', 'x', 'la', 'crisis', 'xi', 'la', 'viuda', 'chermidy', 'xii', 'la', 'guerra', 'xiii', 'el', 'puñal', 'xiv', 'la', 'justicia', 'xv', 'conclusión', 'i', 'el', 'aguinaldo', 'de', 'la', 'duquesa', 'hacia', 'la', 'mitad', 'de', 'la', 'calle', 'de', 'la', 'universidad', 'entre', 'los', 'números', 'y', 'se', 'ven', 'cuatro', 'hoteles', 'que', 'pueden', 'citarse', 'entre', 'los', 'más', 'lindos', 'de', 'parís', 'el', 'primero', 'pertenece', 'al']\n"
          ]
        }
      ],
      "source": [
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "outputId": "75551f7a-a353-478e-e79f-2717646724b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ry41NCO8VaLI"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens: 70327\n",
            "Unique Tokens: 10123\n"
          ]
        }
      ],
      "source": [
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' %len(set(tokens)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "S25NQH-2Vk4t"
      },
      "outputs": [],
      "source": [
        "lines = organize_tokens(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UuT8yCwaVk4u"
      },
      "outputs": [],
      "source": [
        "out_filename = 'pg29640_sequences.txt'\n",
        "save_doc(lines,out_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "outputId": "3ade1612-096d-4e2d-c7eb-a37a791cd45e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU3PfvIZVk4u"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "project gutenberg ebook of germana by edmond about this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever you may copy it give it away or reuse it under the terms of the project gutenberg license included with this ebook or online at\n",
            "gutenberg ebook of germana by edmond about this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever you may copy it give it away or reuse it under the terms of the project gutenberg license included with this ebook or online at wwwgutenbergorg\n",
            "ebook of germana by edmond about this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever you may copy it give it away or reuse it under the terms of the project gutenberg license included with this ebook or online at wwwgutenbergorg title\n",
            "of germana by edmond about this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever you may copy it give it away or reuse it under the terms of the project gutenberg license included with this ebook or online at wwwgutenbergorg title germana\n",
            "germana by edmond about this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever you may copy it give it away or reuse it under the terms of the project gutenberg license included with this ebook or online at wwwgutenbergorg title germana author\n"
          ]
        }
      ],
      "source": [
        "!head -5 pg29640_sequences.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9sjS-WCuVyPb"
      },
      "outputs": [],
      "source": [
        "in_filename = 'pg29640_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "h3LfaT-OVyPb"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "gKYgrO8TVyPb"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "56BLQCHgVyPb"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from numpy import array \n",
        "\n",
        "sequences = array(sequences)\n",
        "X,y=sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y,num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Entrenamos el modelo"
      ],
      "metadata": {
        "id": "MNtk7hdvYxcW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKO9jaEHY1qr",
        "outputId": "503711b2-1994-4780-a843-2cbc48acc02c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "550/550 [==============================] - 15s 16ms/step - loss: 6.9798 - accuracy: 0.0490\n",
            "Epoch 2/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 6.4303 - accuracy: 0.0700\n",
            "Epoch 3/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 6.1261 - accuracy: 0.0784\n",
            "Epoch 4/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 5.9302 - accuracy: 0.0829\n",
            "Epoch 5/100\n",
            "550/550 [==============================] - 9s 15ms/step - loss: 5.7673 - accuracy: 0.0911\n",
            "Epoch 6/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 5.6130 - accuracy: 0.1050\n",
            "Epoch 7/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 5.4682 - accuracy: 0.1148\n",
            "Epoch 8/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 5.3342 - accuracy: 0.1228\n",
            "Epoch 9/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 5.2060 - accuracy: 0.1304\n",
            "Epoch 10/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 5.1177 - accuracy: 0.1362\n",
            "Epoch 11/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 5.0532 - accuracy: 0.1394\n",
            "Epoch 12/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 4.9403 - accuracy: 0.1454\n",
            "Epoch 13/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 4.9127 - accuracy: 0.1437\n",
            "Epoch 14/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 4.7936 - accuracy: 0.1507\n",
            "Epoch 15/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 4.7019 - accuracy: 0.1552\n",
            "Epoch 16/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 4.6194 - accuracy: 0.1586\n",
            "Epoch 17/100\n",
            "550/550 [==============================] - 9s 15ms/step - loss: 4.5373 - accuracy: 0.1621\n",
            "Epoch 18/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 4.4570 - accuracy: 0.1663\n",
            "Epoch 19/100\n",
            "550/550 [==============================] - 9s 15ms/step - loss: 4.3763 - accuracy: 0.1697\n",
            "Epoch 20/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 4.2995 - accuracy: 0.1735\n",
            "Epoch 21/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 4.2206 - accuracy: 0.1778\n",
            "Epoch 22/100\n",
            "550/550 [==============================] - 9s 15ms/step - loss: 4.1456 - accuracy: 0.1826\n",
            "Epoch 23/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 4.0701 - accuracy: 0.1857\n",
            "Epoch 24/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 3.9976 - accuracy: 0.1910\n",
            "Epoch 25/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 3.9273 - accuracy: 0.1974\n",
            "Epoch 26/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 3.8607 - accuracy: 0.2016\n",
            "Epoch 27/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 3.7998 - accuracy: 0.2060\n",
            "Epoch 28/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 3.7356 - accuracy: 0.2136\n",
            "Epoch 29/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 3.6781 - accuracy: 0.2182\n",
            "Epoch 30/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 3.6228 - accuracy: 0.2233\n",
            "Epoch 31/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 3.5621 - accuracy: 0.2314\n",
            "Epoch 32/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 3.5066 - accuracy: 0.2377\n",
            "Epoch 33/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 3.4534 - accuracy: 0.2455\n",
            "Epoch 34/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 3.4036 - accuracy: 0.2511\n",
            "Epoch 35/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 3.3521 - accuracy: 0.2592\n",
            "Epoch 36/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 3.3002 - accuracy: 0.2671\n",
            "Epoch 37/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 3.2504 - accuracy: 0.2746\n",
            "Epoch 38/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 3.2026 - accuracy: 0.2817\n",
            "Epoch 39/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 3.1591 - accuracy: 0.2892\n",
            "Epoch 40/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 3.1139 - accuracy: 0.2965\n",
            "Epoch 41/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 3.0694 - accuracy: 0.3046\n",
            "Epoch 42/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 3.0289 - accuracy: 0.3110\n",
            "Epoch 43/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.9797 - accuracy: 0.3205\n",
            "Epoch 44/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.9406 - accuracy: 0.3258\n",
            "Epoch 45/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.8988 - accuracy: 0.3332\n",
            "Epoch 46/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.8546 - accuracy: 0.3408\n",
            "Epoch 47/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.8220 - accuracy: 0.3490\n",
            "Epoch 48/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.7724 - accuracy: 0.3566\n",
            "Epoch 49/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.7378 - accuracy: 0.3620\n",
            "Epoch 50/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.6989 - accuracy: 0.3694\n",
            "Epoch 51/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.6645 - accuracy: 0.3775\n",
            "Epoch 52/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.6238 - accuracy: 0.3850\n",
            "Epoch 53/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.5880 - accuracy: 0.3909\n",
            "Epoch 54/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.5470 - accuracy: 0.4009\n",
            "Epoch 55/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.5130 - accuracy: 0.4069\n",
            "Epoch 56/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.4744 - accuracy: 0.4138\n",
            "Epoch 57/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.4451 - accuracy: 0.4200\n",
            "Epoch 58/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 2.4080 - accuracy: 0.4279\n",
            "Epoch 59/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.3771 - accuracy: 0.4333\n",
            "Epoch 60/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.3401 - accuracy: 0.4414\n",
            "Epoch 61/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.3046 - accuracy: 0.4483\n",
            "Epoch 62/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.2805 - accuracy: 0.4548\n",
            "Epoch 63/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.2444 - accuracy: 0.4606\n",
            "Epoch 64/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.2140 - accuracy: 0.4668\n",
            "Epoch 65/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.1850 - accuracy: 0.4735\n",
            "Epoch 66/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.1679 - accuracy: 0.4782\n",
            "Epoch 67/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.1194 - accuracy: 0.4888\n",
            "Epoch 68/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.1031 - accuracy: 0.4908\n",
            "Epoch 69/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.0777 - accuracy: 0.4953\n",
            "Epoch 70/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.0349 - accuracy: 0.5056\n",
            "Epoch 71/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 2.0054 - accuracy: 0.5115\n",
            "Epoch 72/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 1.9780 - accuracy: 0.5181\n",
            "Epoch 73/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 1.9536 - accuracy: 0.5221\n",
            "Epoch 74/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 1.9237 - accuracy: 0.5290\n",
            "Epoch 75/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 1.8903 - accuracy: 0.5367\n",
            "Epoch 76/100\n",
            "550/550 [==============================] - 9s 15ms/step - loss: 1.8742 - accuracy: 0.5417\n",
            "Epoch 77/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 1.8463 - accuracy: 0.5463\n",
            "Epoch 78/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 1.8202 - accuracy: 0.5518\n",
            "Epoch 79/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 1.7939 - accuracy: 0.5572\n",
            "Epoch 80/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 1.7682 - accuracy: 0.5632\n",
            "Epoch 81/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 1.7398 - accuracy: 0.5694\n",
            "Epoch 82/100\n",
            "550/550 [==============================] - 9s 15ms/step - loss: 1.7165 - accuracy: 0.5728\n",
            "Epoch 83/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 1.6960 - accuracy: 0.5808\n",
            "Epoch 84/100\n",
            "550/550 [==============================] - 9s 15ms/step - loss: 1.6703 - accuracy: 0.5847\n",
            "Epoch 85/100\n",
            "550/550 [==============================] - 9s 15ms/step - loss: 1.6485 - accuracy: 0.5900\n",
            "Epoch 86/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 1.6147 - accuracy: 0.5979\n",
            "Epoch 87/100\n",
            "550/550 [==============================] - 9s 15ms/step - loss: 1.5976 - accuracy: 0.6026\n",
            "Epoch 88/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 1.5726 - accuracy: 0.6081\n",
            "Epoch 89/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 1.5607 - accuracy: 0.6094\n",
            "Epoch 90/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 1.5255 - accuracy: 0.6178\n",
            "Epoch 91/100\n",
            "550/550 [==============================] - 9s 15ms/step - loss: 1.5067 - accuracy: 0.6218\n",
            "Epoch 92/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 1.4883 - accuracy: 0.6254\n",
            "Epoch 93/100\n",
            "550/550 [==============================] - 9s 15ms/step - loss: 1.4674 - accuracy: 0.6308\n",
            "Epoch 94/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 1.4679 - accuracy: 0.6307\n",
            "Epoch 95/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 1.4142 - accuracy: 0.6439\n",
            "Epoch 96/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 1.4073 - accuracy: 0.6447\n",
            "Epoch 97/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 1.4059 - accuracy: 0.6441\n",
            "Epoch 98/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 1.3498 - accuracy: 0.6606\n",
            "Epoch 99/100\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 1.3356 - accuracy: 0.6629\n",
            "Epoch 100/100\n",
            "550/550 [==============================] - 9s 16ms/step - loss: 1.3296 - accuracy: 0.6625\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa411e809d0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "model = define_model(vocab_size,seq_length)\n",
        "model.fit(X,y,batch_size=128,epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "60o6W_CFY1qr"
      },
      "outputs": [],
      "source": [
        "model.save_weights('./modelej.h5', overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OwKntUAJY1qs"
      },
      "outputs": [],
      "source": [
        "from pickle import dump\n",
        "dump(tokenizer, open('tokenizer.pkl','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "5IaV_UpgdeRp"
      },
      "outputs": [],
      "source": [
        "model = define_model(vocab_size,seq_length)\n",
        "model.load_weights('./modelej.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "zEVZyC8VdeRp"
      },
      "outputs": [],
      "source": [
        "from pickle import load\n",
        "tokenizer = load(open('tokenizer.pkl','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72edc9e4-abec-4f18-e1c2-d6eb2177134a",
        "id": "tKFLG5VIdeRp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y malteses pero todos hacen lo posible por parecer ingleses tenemos también un teatro en el que dan representaciones de juana de arco del maestro verdi yo fui una noche aprovechando que la enferma tenía menos de pulsaciones por minuto al final del primer acto toda la asamblea se levantó respetuosamente\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from random import randint\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e07111e3-363e-474f-e885-8d54d6b65469",
        "id": "BcULPbK-deRq"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "los platos con disgusto desde el momento que los había probado su delgadez era espantosa y la señora chermidy hubiera tenido sumo placer en verla se podía decir que debajo de la piel límpida y transparente no tenía más que huesos y tendones los pómulos parecían salírsele de la cara era\n",
            "\n",
            "preciso que se llama un muchacho guapo quizá también esté la elección de la señora chermidy y todo se le destrozaba el corazón en serio conocer usted paseaba a la muerte de su marido se vive sin arriesgar su boda por un padre de marina se sentaron juntos en una\n"
          ]
        }
      ],
      "source": [
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')\n",
        "generated = generate_seq(model,tokenizer,seq_length,seed_text,50)\n",
        "print(generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG188-6TI3ph"
      },
      "source": [
        "Recuerda guardar este notebook en tu repositorio usando la opción \"Save in GitHub\" del menú File."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "practica9_3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}